# 2025-01-05

Sunday:
  * Mostly spent running errands.
  * Tech work, but focused on work work rather than personal projects, so can't type about them. Note that now that I am about to head back to work after 2 week hiatus, I expect that writing will tail off.

# Unmasking AI

Nearly finished reading Unmasking AI. 

General theme - AI models have been trained and built by teams with demographics which are more limited than the general population. These then do not perform well on people outside of that demographic slice - e.g. perform better worse for women, or people of African descent, or even different skin tones. It takes other people outside of dev group - and potentially people in the underrepresented demographics - to discover the flaws in the model. These flaws in the model could also end up penalizing groups who are already marginalized, further exacerbating problems.

The book did a good covering a number of possible reasons for why this situation happened, although could have been a bit more explicit about listing them:
  * Limited demographic diversity of team leading to not testing/validating enough.
  * Privacy concerns around using faces from private people led to training corpus being comprised of public figures like politicians. This data was also less diverse than general population.
  * Instructions for labeling data potentially introduces skew. E.g. some had labelers/annotators only specify Male or Female which doesn't work as well for people who don't present in those buckets. Choosing race versus skin pigmentation also tricky.
  * Lack of benchmarks or validation tests for facial recongition also hurt - as does heavy amount of secrecy around the training data and evaluation criteria.

The author also tied the general industry issues to her own personal experiences - such as not being recognized by models without using a white mask, or her testimony in front of Congress. This made it a more personal narrative, but also at times felt like the same stories were being reused multiple times - and that some molehills were being made into mountains.

There were pretty unclear conclusions in the text - or at least examples - of where the bad models could cause harm rather than just being wrong. Many were hypothesized, and a few examples were given of people incorrectly arrested, but more details here would have driven home whether this was a technical glitch that needed to be debugged, or had real societal impact.

----

So, in summary I think that the book:
  * Told an interesting personal narrative
  * Provided evidence of how data used in training models was not representative of all of society
  * Gave some potential reasons for why the under-representation happened.
  * Was a bit less clear about societal impact of those flaws.
  * Didn't really cover cases as much of where existing biases fed into models and whether anything needed to be done about it

Was a good read, but not an amazing read.