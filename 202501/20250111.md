# 2025-01-11

As expected, I didnt' update this diary during the work-week. I tend to work around 50-60 hours (especially factoring in commute time) and don't have substantial time or mental energy to focus on tech pieces or diarying during the week.

# RL - Monte Carlo Methods

Re-read more of Chapter 5 of Barto Sutton focused on Monte Carlo methods.

Other resources: 
  * Compact video about the concepts: https://www.youtube.com/watch?v=bpUszPiWM7o. It doesn't go super deeply into any of the details, but is a concentrated punch. 
  * David Silver's Lecture 4, which also includes TD Learning: https://www.youtube.com/watch?v=PnHCvfgC_ZA&t=3089s. Very much like Barto+Sutton (with examples directly there) but sometimes lecture form helps. Also focused on combining both methods into value function estimation, with a follow-up lecture on control/policy adjustment.

This chapter moved from the model-based dynamic programming optimizing for an MDP to a model-free approach for value-function and/or state-action estimation and policy choice.

A bunch of open questions from me;
  - To explore full state-action space with on-policy methods, the book focused on epislon-soft policies such as epsilon-greedy to provide some level of exploration instead of purely determinstic policies. It seems like an ideal approach would be if not going the exploitation route, to choose the least explored action for a state rather than do a uniform sample across all possible actions for a state as shown with epsilon-greedy. In infinite samples it doesn't matter, but this might provide more data over other locations.
  - The video and the book covered incremental learning (rather than average over all sessions) using a fixed alpha learning rate. But it seems like having learning step be large in the beginning and slowly get smaller would be better. More standard supervised learning techniques typically provide a decay function like this.
  - Motivation for off-policy learning is better explained in the compact video than in Barto-Sutton - effectively we can take many examples used with non-optimal policies or multiple policies and still apply them to the actual target policy.
  - Importance Sampling for off-policy methods is interesting because it allows us to continue with observed trajectories instead of a model. One thing that was curious was if the target policy is deterministic, the numerator seems like it would be either 1 or 0 - at each state there would only be one optimal action, and unless the trajectory includes those state-action pairs at least one of the probabilities would be 0. But maybe I'm missing something.
  - I'm not quite sure why first-state versus all-state sampling is more important for trajectories from underlying cyclic MDPs. It seems like more information is better, so why not use all states?
  - Similarly, the bias/variance tradeoff of simple averages versus weighted averges wasn't extremely clear to me.
  

  https://github.com/Duane321/mutual_information/tree/main/videos/monte_carlo_for_RL_and_off_policy_methods includes source code for the off policy blackjack tutorial and is probably worth understanding.