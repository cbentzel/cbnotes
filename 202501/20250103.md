# 2025-01-03

## RL

Going back to learning material, but I do need to run through some programming exercises.

Currently watching Lecture 4 of David Silver's RL course. This is focused on model-free value prediction from a fixed policy.

Idea is that we don't want to actually model the full MDP, but still understand value function for a specific policy via sampling (monte-carlo policy evaluation, or TD). 

Monte-carlo depends on assessing state by total return at the end of an episode. TD is more focused on something closer to A* estimates of return - or maybe that's what the Bellamn equation looked like.

I found this a little less clear than reading, and will go to reading chapter 5+6 of Barto+Sutton instead.

