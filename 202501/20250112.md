# 2025-01-12

## RL

Focus today was going back over TD learning - both TD(0) and TD(lambda) for arbitratry timestamps.

* Chapter 6 of Barto Sutton
* Compact "Mutual Information" video: https://www.youtube.com/watch?v=AJiG3ykOxmY
* Lecture [4](https://www.youtube.com/watch?v=PnHCvfgC_ZA) and [5](https://www.youtube.com/watch?v=0g4j2k_Ggc4) of David Silver's RL course.

Unlike Monte-Carlo, TD does not need to wait until end of an episode. Monte Carlo can actually be seen as an extreme version of TD learning as well of infinite steps. TD(lambda) allows going between them.

In general TD(0) is a maximum likelihood estimator of the MDP. Monte Carlo minimizes RMS error on existing data. Generally TD(0) or small numbers learn faster. Both should converge ultimately.

* Watching this video as well https://www.youtube.com/watch?v=AJiG3ykOxmY